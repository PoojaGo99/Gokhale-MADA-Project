dev.off()
# Create a vector to store the results from the leave-one-out analysis
leave_one_out_results <- data.frame(study = data$study, RR = NA, CI_lower = NA, CI_upper = NA)
# Perform leave-one-out analysis by excluding each study one at a time
for (i in 1:nrow(data)) {
# Exclude the i-th study
leave_one_out_data <- data[-i, ]
# Perform meta-analysis on the reduced data
meta_analysis_loo <- rma(yi = log_rr, sei = sqrt(var_log_rr), data = leave_one_out_data, method = "DL")
# Store the results (RR and CI) for each study excluded
leave_one_out_results$RR[i] <- exp(meta_analysis_loo$beta)  # Exponentiate to get RR
leave_one_out_results$CI_lower[i] <- exp(meta_analysis_loo$ci.lb)  # Lower CI bound
leave_one_out_results$CI_upper[i] <- exp(meta_analysis_loo$ci.ub)  # Upper CI bound
}
# View the leave-one-out results
print(leave_one_out_results)
# Funnel Plot
funnel(meta_analysis, main="Funnel Plot", xlab="Log Risk Ratio", ylab="Standard Error", col="blue")
# Perform Egger's Test for Small-Study Effects
egger_test <- regtest(meta_analysis, model = "lm")
# Print the results of Egger's Test
cat("Egger's Test p-value:", round(egger_test$pval, 4), "\n")
# Load necessary libraries
library(metafor)
library(dplyr)
# Example dataset
data = data.frame(
study = c("Galsky, 2020b", "Bellmunt, 2021", "Powles, 2021", "Apolo, 2025", "Bajorin, 2021", "Bellmunt, 2017", "Powles, 2020"),
events_ici = c(17, 17, 10, 26, 33, 10, 16),
n_ici = c(354, 390, 302, 330, 351, 266, 344),
events_comparator = c(7, 0, 0, 0, 3, 1, 0),
n_comparator = c(390, 397, 342, 348, 348, 255, 345)
)
# Add correction if there are zero events (continuity correction)
data <- data %>%
mutate(
correction = ifelse(events_ici == 0 | events_comparator == 0, 0.5, 0),
events_ici = events_ici + correction,
events_comparator = events_comparator + correction,
n_ici = n_ici + correction,
n_comparator = n_comparator + correction
)
# Compute Risk Ratio (RR), logRR and variance
data <- data %>%
mutate(
rr = (events_ici / n_ici) / (events_comparator / n_comparator),  # Risk Ratio
log_rr = log(rr),  # Log Risk Ratio
var_log_rr = (1 / events_ici) + (1 / events_comparator) - (1 / n_ici) - (1 / n_comparator)  # Variance of log(RR)
)
# Remove any problematic values (if any)
data <- na.omit(data) %>% filter(!is.infinite(var_log_rr) & !is.na(var_log_rr))
# Perform random-effects meta-analysis
meta_analysis <- rma(yi = log_rr, sei = sqrt(var_log_rr), data = data, method = "REML")
#Prediction interval
pred_interval <- predict(meta_analysis)
lower_PI <- pred_interval$pi.lb  # Correct lower bound of PI
upper_PI <- pred_interval$pi.ub  # Correct upper bound of PI
exp_lower_PI <- exp(lower_PI)
exp_upper_PI <- exp(upper_PI)
log_exp_lower_PI = log(exp_lower_PI)
log_exp_upper_PI = log(exp_upper_PI)
# Display meta-analysis results
summary(meta_analysis)
# Calculate weights for each study based on the inverse of the variances
data$weight <- 1 / data$var_log_rr
# Calculate weights as percentages
data$weight_percent <- (data$weight / sum(data$weight)) * 100
# Add % symbol to weights
data$weight_label <- paste0(round(data$weight_percent, 2), "%")
cat("\nðŸ”¹ Heterogeneity Testing ðŸ”¹\n")
cat("IÂ²:", round(meta_analysis$I2, 2), "%\n")  # Higgins' IÂ²
cat("Q-test p-value:", round(meta_analysis$QEp, 4), "\n")  # Cochran's Q-test
cat("TauÂ²:", round(meta_analysis$tau2, 4), "\n")  # TauÂ²
# Total events across all studies
total_ici_events <- sum(data$n_ici)
total_comparator_events <- sum(data$n_comparator)
# Reset plotting layout to default (1 plot per window)
par(mfrow = c(1, 1))  # This ensures one plot at a time
png("Hyper.png", width=2000, height=1200, res=300)
# Create the forest plot
forest(meta_analysis,
main="Hyperthyroidism",
slab = data$study,      # Add study names
xlab = "Risk Ratio",    # Label for x-axis
atransf = exp,          # Exponentiate the estimates (logRR to RR)
xlim = c(-25, 20),      # Set a reasonable x-axis range based on CI
ylim = c(10,-3),
ilab = cbind(data$events_ici, data$n_ici, data$events_comparator, data$n_comparator, data$weight_label),  # Add event counts, weights as labels
ilab.xpos = c(-12, -9, -6, -3, 8.5),  # Adjust position of event count and weight labels
refline = 0,            # Reference line at RR = 1
annotate = TRUE,        # Annotate the plot with values
psize = 1.2,            # Point size for the forest plot
col = "black",           # Color of the points and labels
cex = 0.8,              # Font size for the labels
font = 2)               # Font style for the labels
#Add labels
text(x = -23, y = length(data$study) + 2, labels = "Study", cex = 1, pos = 3)  # "Study" label at the top
text(x = -11.5, y = length(data$study) + 2, labels = "ICI", cex = 1, pos = 3)  # "ICI" label for ICI
text(x = -12.5, y = length(data$study) + 1, labels = "Events", cex = 1, pos = 3)  # "No. of Events" for ICI
text(x = -9.25, y = length(data$study) + 1, labels = "Total", cex = 1, pos = 3)
text(x = -4.5, y = length(data$study) + 2, labels = "Comparator", cex = 1, pos = 3)  # "Comparator" at the top
text(x = -5.75, y = length(data$study) + 1, labels = "Events", cex = 1, pos = 3)  # "No. of Events" for Comparator
text(x = -2.5, y = length(data$study) + 1, labels = "Total", cex = 1, pos = 3)  # "Total Events" for Comparator
text(x = 8, y = length(data$study) + 2, labels = "Weight", cex = 1, pos = 3)  # "Weight" header at the top
text(x = 16, y = length(data$study) + 2, labels = "RR (95% CI)", cex = 1, pos = 3)  # "Weight" header at the top
segments(x0 = log_exp_lower_PI, x1 = log_exp_upper_PI,
y0 = length(data$study) -9, y1 = length(data$study) -9,  # Adjust y position to be below the studies
col = "red", lwd = 2)
# Adding the text label for the PI (adjusting x and y positions as needed)
text(x = -17.8,  # Position the text in the center of the PI segment
y = length(data$study) - 9,           # Slightly below the PI line
labels = sprintf("Prediction Interval: %.2f - %.2f", exp_lower_PI, exp_upper_PI),
cex = 0.8, col = "black", family = "sans")
text(x=-16.2,
y = length(data$study) - 10,  # Adjust y to place it below the studies
labels = sprintf("Heterogeneity: IÂ² = %.2f%%, p = %.4f",
meta_analysis$I2,
meta_analysis$QEp),
cex = 0.8, col = "black", family = "sans")
# Display total ICI and comparator events
text(x = -9, y = -1, labels = total_ici_events, cex = 0.8, font=2)
text(x = -3, y = -1, labels = total_comparator_events, cex = 0.8, font=2)
dev.off()
# Create a vector to store the results from the leave-one-out analysis
leave_one_out_results <- data.frame(study = data$study, RR = NA, CI_lower = NA, CI_upper = NA)
# Perform leave-one-out analysis by excluding each study one at a time
for (i in 1:nrow(data)) {
# Exclude the i-th study
leave_one_out_data <- data[-i, ]
# Perform meta-analysis on the reduced data
meta_analysis_loo <- rma(yi = log_rr, sei = sqrt(var_log_rr), data = leave_one_out_data, method = "DL")
# Store the results (RR and CI) for each study excluded
leave_one_out_results$RR[i] <- exp(meta_analysis_loo$beta)  # Exponentiate to get RR
leave_one_out_results$CI_lower[i] <- exp(meta_analysis_loo$ci.lb)  # Lower CI bound
leave_one_out_results$CI_upper[i] <- exp(meta_analysis_loo$ci.ub)  # Upper CI bound
}
# View the leave-one-out results
print(leave_one_out_results)
# Funnel Plot
funnel(meta_analysis, main="Funnel Plot", xlab="Log Risk Ratio", ylab="Standard Error", col="blue")
# Perform Egger's Test for Small-Study Effects
egger_test <- regtest(meta_analysis, model = "lm")
# Print the results of Egger's Test
cat("Egger's Test p-value:", round(egger_test$pval, 4), "\n")
# Load necessary libraries
library(metafor)
library(dplyr)
# Example dataset
data = data.frame(
study = c("Galsky, 2020b", "Bellmunt, 2021", "Powles, 2021", "Bajorin, 2021", "Bellmunt, 2017", "Powles, 2020"),
events_ici = c(2, 7, 5, 7, 6, 3),
n_ici = c(354, 390, 302, 351, 266, 344),
events_comparator = c(1, 6, 2, 3, 1, 0),
n_comparator = c(390, 397, 342, 348, 255, 345)
)
# Add correction if there are zero events (continuity correction)
data <- data %>%
mutate(
correction = ifelse(events_ici == 0 | events_comparator == 0, 0.5, 0),
events_ici = events_ici + correction,
events_comparator = events_comparator + correction,
n_ici = n_ici + correction,
n_comparator = n_comparator + correction
)
# Compute Risk Ratio (RR), logRR and variance
data <- data %>%
mutate(
rr = (events_ici / n_ici) / (events_comparator / n_comparator),  # Risk Ratio
log_rr = log(rr),  # Log Risk Ratio
var_log_rr = (1 / events_ici) + (1 / events_comparator) - (1 / n_ici) - (1 / n_comparator)  # Variance of log(RR)
)
# Remove any problematic values (if any)
data <- na.omit(data) %>% filter(!is.infinite(var_log_rr) & !is.na(var_log_rr))
# Perform random-effects meta-analysis
meta_analysis <- rma(yi = log_rr, sei = sqrt(var_log_rr), data = data, method = "REML")
#Prediction interval
pred_interval <- predict(meta_analysis)
lower_PI <- pred_interval$pi.lb  # Correct lower bound of PI
upper_PI <- pred_interval$pi.ub  # Correct upper bound of PI
exp_lower_PI <- exp(lower_PI)
exp_upper_PI <- exp(upper_PI)
# Display meta-analysis results
summary(meta_analysis)
# Calculate weights for each study based on the inverse of the variances
data$weight <- 1 / data$var_log_rr
# Calculate weights as percentages
data$weight_percent <- (data$weight / sum(data$weight)) * 100
# Add % symbol to weights
data$weight_label <- paste0(round(data$weight_percent, 2), "%")
cat("\nðŸ”¹ Heterogeneity Testing ðŸ”¹\n")
cat("IÂ²:", round(meta_analysis$I2, 2), "%\n")  # Higgins' IÂ²
cat("Q-test p-value:", round(meta_analysis$QEp, 4), "\n")  # Cochran's Q-test
cat("TauÂ²:", round(meta_analysis$tau2, 4), "\n")  # TauÂ²
# Total events across all studies
total_ici_events <- sum(data$n_ici)
total_comparator_events <- sum(data$n_comparator)
# Reset plotting layout to default (1 plot per window)
par(mfrow = c(1, 1))  # This ensures one plot at a time
png("C.png", width = 2000, height = 1200, res = 300)
# Create the forest plot
forest(meta_analysis,
main="Colitis",
slab = data$study,      # Add study names
xlab = "Risk Ratio",    # Label for x-axis
atransf = exp,          # Exponentiate the estimates (logRR to RR)
xlim = c(-25, 20),      # Set a reasonable x-axis range based on CI
ylim = c(9,-3),
ilab = cbind(data$events_ici, data$n_ici, data$events_comparator, data$n_comparator, data$weight_label),  # Add event counts, weights as labels
ilab.xpos = c(-12, -9, -6, -3, 8.5),  # Adjust position of event count and weight labels
refline = 0,            # Reference line at RR = 1
annotate = TRUE,        # Annotate the plot with values
psize = 1.2,            # Point size for the forest plot
col = "black",           # Color of the points and labels
cex = 0.8,              # Font size for the labels
font = 2)               # Font style for the labels
#Add labels
text(x = -23, y = length(data$study) + 2, labels = "Study", cex = 1, pos = 3)  # "Study" label at the top
text(x = -11.5, y = length(data$study) + 2, labels = "ICI", cex = 1, pos = 3)  # "ICI" label for ICI
text(x = -12.5, y = length(data$study) + 1, labels = "Events", cex = 1, pos = 3)  # "No. of Events" for ICI
text(x = -9.25, y = length(data$study) + 1, labels = "Total", cex = 1, pos = 3)
text(x = -4.5, y = length(data$study) + 2, labels = "Comparator", cex = 1, pos = 3)  # "Comparator" at the top
text(x = -5.75, y = length(data$study) + 1, labels = "Events", cex = 1, pos = 3)  # "No. of Events" for Comparator
text(x = -2.5, y = length(data$study) + 1, labels = "Total", cex = 1, pos = 3)  # "Total Events" for Comparator
text(x = 8, y = length(data$study) + 2, labels = "Weight", cex = 1, pos = 3)  # "Weight" header at the top
text(x = 16, y = length(data$study) + 2, labels = "RR (95% CI)", cex = 1, pos = 3)  # "Weight" header at the top
# Exponentiate the prediction interval bounds if you're plotting on log scale
log_exp_lower_PI = log(exp_lower_PI)
log_exp_upper_PI = log(exp_upper_PI)
segments(x0 = log_exp_lower_PI, x1 = log_exp_upper_PI,
y0 = length(data$study) -7.75, y1 = length(data$study) -7.75,  # Adjust y position to be below the studies
col = "red", lwd = 2)
# Adding the text label for the PI (adjusting x and y positions as needed)
text(x = -18,  # Position the text in the center of the PI segment
y = length(data$study) - 7.75,           # Slightly below the PI line
labels = sprintf("Prediction Interval: %.2f - %.2f", exp_lower_PI, exp_upper_PI),
cex = 0.8, col = "black", family = "sans")
text(x=-16.4,
y = length(data$study) - 8.5,  # Adjust y to place it below the studies
labels = sprintf("Heterogeneity: IÂ² = %.2f%%, p = %.4f",
meta_analysis$I2,
meta_analysis$QEp),
cex = 0.8, col = "black", family = "sans")
# Display total ICI and comparator events
text(x = -9, y = -1, labels = total_ici_events, cex = 0.8, font=2)
text(x = -3, y = -1, labels = total_comparator_events, cex = 0.8, font=2)
dev.off()
cat("\nðŸ”¹ Heterogeneity Testing ðŸ”¹\n")
cat("IÂ²:", round(meta_analysis$I2, 2), "%\n")  # Higgins' IÂ²
cat("Q-test p-value:", round(meta_analysis$QEp, 4), "\n")  # Cochran's Q-test
cat("TauÂ²:", round(meta_analysis$tau2, 4), "\n")  # TauÂ²
# Create a vector to store the results from the leave-one-out analysis
leave_one_out_results <- data.frame(study = data$study, RR = NA, CI_lower = NA, CI_upper = NA)
# Perform leave-one-out analysis by excluding each study one at a time
for (i in 1:nrow(data)) {
# Exclude the i-th study
leave_one_out_data <- data[-i, ]
# Perform meta-analysis on the reduced data
meta_analysis_loo <- rma(yi = log_rr, sei = sqrt(var_log_rr), data = leave_one_out_data, method = "DL")
# Store the results (RR and CI) for each study excluded
leave_one_out_results$RR[i] <- exp(meta_analysis_loo$beta)  # Exponentiate to get RR
leave_one_out_results$CI_lower[i] <- exp(meta_analysis_loo$ci.lb)  # Lower CI bound
leave_one_out_results$CI_upper[i] <- exp(meta_analysis_loo$ci.ub)  # Upper CI bound
}
# View the leave-one-out results
print(leave_one_out_results)
# Funnel Plot
funnel(meta_analysis, main="Funnel Plot", xlab="Log Risk Ratio", ylab="Standard Error", col="blue")
# Perform Egger's Test for Small-Study Effects
egger_test <- regtest(meta_analysis, model = "lm")
# Print the results of Egger's Test
cat("Egger's Test p-value:", round(egger_test$pval, 4), "\n")
#| label: fig-processing1
#| fig-cap: "Study Flowchart"
#| echo: FALSE
knitr::include_graphics(here("assets","Figure1.png"))
# load a few R packages
library(here)
library(huxtable)
library(knitr)
#| label: fig-processing1
#| fig-cap: "Study Flowchart"
#| echo: FALSE
knitr::include_graphics(here("assets","Figure1.png"))
#| label: tbl-summarytable
#| tbl-cap: "Descriptive Statistics"
#| echo: FALSE
#| warning: FALSE
#| message: FALSE
library(flextable)
# Load and modify table (all processing hidden)
demographic_stats <- readRDS(here::here("results", "tables", "summarytable.rds"))[-c(1, 2), ]
# Create and format flextable (visible in output only)
desc_stats_table <- flextable(demographic_stats) |>
# Borders
border_outer(part = "all") |>
border_inner(part = "all") |>
# Formatting
bold(part = "header") |>
fontsize(size = 10, part = "all") |>
color(color = "black", part = "all") |>
# Layout
set_table_properties(width = 1, layout = "autofit") |>  # Full width
align(align = "center", part = "header") |>
padding(padding = 4, part = "all")
# Display table
desc_stats_table
#| label: tbl-randomforest
#| tbl-cap: "Random Forest Evaluation Metrics"  # Removed "Table X" to prevent duplication
#| echo: FALSE
#| warning: FALSE
#| message: FALSE
library(flextable)
library(officer)
# Load and prepare data
rf_metrics <- readRDS("../../results/tables/model_table.rds")
rf_metrics_cleaned <- rf_metrics[-1, ]  # Removes the second row
flextable(rf_metrics_cleaned) |>
bold(part = "header") |>
border(part = "all", border = fp_border(color = "black", width = 1)) |>
colformat_double(digits = 3) |>
align(align = "center", part = "header") |>
align(align = "right", part = "body", j = which(sapply(rf_metrics_cleaned, is.numeric))) |>
set_table_properties(width = 1, layout = "autofit") |>
width(width = c(1.8, rep(1.2, ncol(rf_metrics_cleaned)-1))) |>
bg(part = "header", bg = "#f2f2f2") |>
fontsize(size = 10, part = "all") |>
padding(padding = 4, part = "all")
#| label: tbl-randomforest
#| tbl-cap: "Random Forest Evaluation Metrics"  # Removed "Table X" to prevent duplication
#| echo: FALSE
#| warning: FALSE
#| message: FALSE
library(flextable)
library(officer)
# Load and prepare data
rf_metrics <- readRDS("../../results/tables/model_table.rds")
rf_metrics_cleaned <- rf_metrics[-1, ]  # Removes the second row
flextable(rf_metrics_cleaned) |>
bold(part = "header") |>
border(part = "all", border = fp_border(color = "black", width = 1)) |>
colformat_double(digits = 3, na_str = "-") |>
align(align = "center", part = "header") |>
align(align = "right", part = "body", j = which(sapply(rf_metrics_cleaned, is.numeric))) |>
set_table_properties(width = 1, layout = "autofit") |>
width(width = c(1.8, rep(1.2, ncol(rf_metrics_cleaned)-1))) |>
bg(part = "header", bg = "#f2f2f2") |>
fontsize(size = 10, part = "all") |>
padding(padding = 4, part = "all")
library(dplyr)
library(flextable)
# Load and clean data
rf_metrics <- readRDS("../../results/tables/model_table.rds")
rf_metrics_cleaned <- rf_metrics[-1, ]
# Convert numeric-looking columns to double (if needed)
rf_metrics_cleaned <- rf_metrics_cleaned %>%
mutate(across(where(~ all(grepl("^[-+]?[0-9]*\\.?[0-9]+$", .))), as.numeric))
# Build styled flextable
flextable(rf_metrics_cleaned) |>
bold(part = "header") |>
border(part = "all", border = fp_border(color = "black", width = 1)) |>
colformat_double(digits = 3, na_str = "-") |>
align(align = "center", part = "header") |>
align(
align = "right",
part = "body",
j = which(sapply(rf_metrics_cleaned, is.numeric))
) |>
set_table_properties(width = 1, layout = "autofit") |>
width(width = c(1.8, rep(1.2, ncol(rf_metrics_cleaned) - 1))) |>
bg(part = "header", bg = "#f2f2f2") |>
fontsize(size = 10, part = "all") |>
padding(padding = 4, part = "all")
#| label: tbl-randomforest
#| tbl-cap: "Random Forest Evaluation Metrics"  # Removed "Table X" to prevent duplication
#| echo: FALSE
#| warning: FALSE
#| message: FALSE
library(dplyr)
library(flextable)
# Load and clean data
rf_metrics <- readRDS("../../results/tables/model_table.rds")
rf_metrics_cleaned <- rf_metrics[-1, ]
# Convert numeric-looking columns to double (if needed)
rf_metrics_cleaned <- rf_metrics_cleaned %>%
mutate(across(where(~ all(grepl("^[-+]?[0-9]*\\.?[0-9]+$", .))), as.numeric))
# Build styled flextable
flextable(rf_metrics_cleaned) |>
bold(part = "header") |>
border(part = "all", border = fp_border(color = "black", width = 1)) |>
colformat_double(digits = 3, na_str = "-") |>
align(align = "center", part = "header") |>
align(
align = "left",
part = "body",
j = which(sapply(rf_metrics_cleaned, is.numeric))
) |>
set_table_properties(width = 1, layout = "autofit") |>
width(width = c(1.8, rep(1.2, ncol(rf_metrics_cleaned) - 1))) |>
bg(part = "header", bg = "#f2f2f2") |>
fontsize(size = 10, part = "all") |>
padding(padding = 4, part = "all")
#load needed packages. make sure they are installed.
#install.packages("gt")
#install.packages("emmeans")
library(ggplot2) #for plotting
library(broom) #for cleaning up output from lm()
library(here) #for data loading/saving
library(tidyverse)
library(tidymodels)
library(gt)
library(stringr)
library(patchwork)
library(emmeans)
library(sf) #for geospatial analysis
library(spdep)
library(spatialreg)
library(vip) # For variable importance plots
library(dplyr)
library(tidytext)
library(forcats)
#load needed packages. make sure they are installed.
#install.packages("gt")
#install.packages("emmeans")
library(ggplot2) #for plotting
library(broom) #for cleaning up output from lm()
library(here) #for data loading/saving
library(tidyverse)
library(tidymodels)
library(gt)
library(stringr)
library(patchwork)
library(emmeans)
library(sf) #for geospatial analysis
library(spdep)
library(spatialreg)
library(vip) # For variable importance plots
library(dplyr)
library(tidytext)
library(forcats)
#path to data
data_location <- here::here("data","processed-data","eda_outputs.rds")
#load data.
final_processed_data <- readRDS(data_location)
# Extract key datasets
processed_data_no_percent <- final_processed_data[["processed_data_no_percent"]]
county_race_sex <- final_processed_data[["county_race_sex"]]
county_race <- final_processed_data[["county_race"]]
county_sex <- final_processed_data[["county_sex"]]
county_overall <- final_processed_data[["county_overall"]]
#load needed packages. make sure they are installed.
#install.packages("gt")
#install.packages("emmeans")
library(ggplot2) #for plotting
library(broom) #for cleaning up output from lm()
library(here) #for data loading/saving
library(tidyverse)
library(tidymodels)
library(gt)
library(stringr)
library(patchwork)
library(emmeans)
library(sf) #for geospatial analysis
library(spdep)
library(spatialreg)
library(vip) # For variable importance plots
library(dplyr)
library(tidytext)
library(forcats)
# Checking the relationship between mortality and year
lm_model_year <- lm(mortalities ~ Year, data = county_overall)
lm_results_year <- broom::tidy(lm_model_year)
# Display and save the regression results
print(lm_results_year)
saveRDS(lm_results_year, file = here::here("products", "manuscript", "supplement", "lm_results_year.rds"))
# Extract model fit statistics
model_fit_lm <- broom::glance(lm_model_year) %>%
dplyr::mutate(model = "LM: National Year Trend")
# Save as table
saveRDS(model_fit_lm, here::here("results", "tables", "model_fit_national.rds"))
# Plot linear regression results
lm_plot_year <- ggplot(county_overall, aes(x = Year, y = mortalities)) +
geom_point(alpha = 0.5) +
geom_smooth(method = "lm", se = FALSE, color = "red") +
theme_minimal() +
labs(title = "Linear Regression: Mortality Rates Over Time", x = "Year", y = "Mortality Rate")
print(lm_plot_year)
ggsave(here::here("products", "manuscript", "supplement", "lm_plot_year.png"), plot = lm_plot_year)
#load needed packages. make sure they are installed.
#install.packages("gt")
#install.packages("emmeans")
library(ggplot2) #for plotting
library(broom) #for cleaning up output from lm()
library(here) #for data loading/saving
library(tidyverse)
library(tidymodels)
library(gt)
library(stringr)
library(patchwork)
library(emmeans)
library(sf) #for geospatial analysis
library(spdep)
library(spatialreg)
library(vip) # For variable importance plots
library(dplyr)
library(tidytext)
library(forcats)
# Plot linear regression results
lm_plot_year <- ggplot(county_overall, aes(x = Year, y = mortalities)) +
geom_point(alpha = 0.5) +
geom_smooth(method = "lm", se = FALSE, color = "red") +
theme_minimal() +
labs(title = "Linear Regression: Mortality Rates Over Time", x = "Year", y = "Mortality Rate")
